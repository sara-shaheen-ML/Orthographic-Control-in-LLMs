Orthographic Control in Large Language Models
Teaching Token-Based LLMs to Spell at the Character Level

Large Language Models can generate full reports, structured code, and coherent long-form reasoning â€” yet they may fail at a task taught in early primary school: spelling a single word correctly, letter by letter.

This repository accompanies our research on:

Why token-based LLMs struggle with character-level orthographic precision, and how far parameter-efficient fine-tuning (LoRA) can push this capability.

ğŸ” Motivation

LLMs are optimized for contextual fluency, not exact character reconstruction.

This leads to surprising behaviour:

spell: necessary
â†’ n e c e s a r y   âŒ

spell: accommodation
â†’ a c o m o d a t i o n   âŒ


This is not a reasoning failure â€” it is a representation and tokenization limitation.

However, in domains such as:

early literacy

spelling instruction

educational assessment

controlled generation

character-level accuracy is the task itself.

â“ Research Questions

Can token-based LLMs reliably perform character-level spelling?

Is the limitation caused by tokenization and contextual decoding?

Can parameter-efficient fine-tuning (LoRA) improve orthographic control without full retraining?

âœ¨ Contributions

âœ” A controlled evaluation pipeline for LLM spelling
âœ” Character-level orthographic metrics
âœ” Exact-match and error analysis
âœ” LoRA fine-tuning for spelling adaptation
âœ” Empirical evidence of:

significant improvement after PEFT

persistent unsolved failure modes

This shows that:

Orthographic knowledge exists in LLMs â€” but its externalization is unstable.

ğŸ§  Method Overview

We evaluate spelling generation using:

1ï¸âƒ£ Prompt-based inference

Zero-shot

Few-shot

2ï¸âƒ£ Parameter-efficient fine-tuning

LoRA adaptation

Low training cost

No full model retraining

3ï¸âƒ£ Evaluation metrics

Exact Match Accuracy

Character Error Rate (CER)

Position-wise character accuracy

Word-length generalization

ğŸ“Š Example Outcome

LoRA significantly:

improves exact-match spelling

reduces character-level errors

stabilizes output format

But:

â— The problem is not completely solved, revealing a deeper modeling limitation.

ğŸ“‚ Repository Structure
.
â”œâ”€â”€ notebooks/        # Main experimental pipeline
â”œâ”€â”€ data/             # Training and evaluation word lists
â”œâ”€â”€ src/              # Training and evaluation scripts (optional modular version)
â”œâ”€â”€ results/          # Output metrics and logs
â”œâ”€â”€ figures/          # Plots used in the paper
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Installation
git clone https://github.com/YOUR_USERNAME/orthographic-control-llm.git
cd orthographic-control-llm

pip install -r requirements.txt

â–¶ï¸ Quick Start

Run the main experiment:

jupyter notebook notebooks/your_notebook.ipynb


or (if modular scripts are used):

python src/run_evaluation.py

ğŸ§ª Reproducing the Experiments

Prepare the dataset

Run baseline prompting evaluation

Fine-tune using LoRA

Run post-training evaluation

All steps are provided in the notebook for full reproducibility.

ğŸ¯ Key Insight

Scaling model size does not guarantee fine-grained control.

This project highlights a fundamental gap between:

semantic fluency
and

symbolic precision

ğŸ« Why This Matters

This research is particularly relevant for:

AI for education

controllable text generation

human-AI alignment at the symbolic level

tokenization design

character-aware language modeling

ğŸ”— Paper & Citation

If you use this code, please cite:

@article{shaheen2026orthographic,
  title   = {Orthographic Control in Token-Based Large Language Models},
  author  = {Shaheen, Sara},
  year    = {2026}
}

ğŸ¤ Collaboration

I am currently expanding this research toward:

character-aware decoding strategies

hybrid token/character representations

educational LLM applications

If you are working in:

LLM controllability

PEFT methods

NLP for education

letâ€™s collaborate.

ğŸ“¬ Author

Dr. Sara Shaheen
PhD in Computer Science â€“ Artificial Intelligence
AI Researcher | LLMs | Computer Vision | AI for Education

Google Scholar: https://scholar.google.com/citations?user=pBgJOiEAAAAJ

â­ If you find this useful

Star the repo, open an issue, or try the spelling test yourself ğŸ˜„
